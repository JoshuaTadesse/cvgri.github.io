<!DOCTYPE html><!--KeRCacRKoUFGG9nje_rBS--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/Research Project Images/Diffusion Models in the Medical Field.png"/><link rel="stylesheet" href="/cvgri.github.io/_next/static/css/c83f8f8090aa4c07.css" data-precedence="next"/><link rel="stylesheet" href="/cvgri.github.io/_next/static/css/322b103938224f59.css" data-precedence="next"/><link rel="stylesheet" href="/cvgri.github.io/_next/static/css/48a2b02155d2754b.css" data-precedence="next"/><link rel="stylesheet" href="/cvgri.github.io/_next/static/css/cdbf817f0b6ed54d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/cvgri.github.io/_next/static/chunks/webpack-fabe833255cd74ec.js"/><script src="/cvgri.github.io/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/cvgri.github.io/_next/static/chunks/255-ed3fa00e237dff20.js" async=""></script><script src="/cvgri.github.io/_next/static/chunks/main-app-c40da99ebe4848d2.js" async=""></script><script src="/cvgri.github.io/_next/static/chunks/394-dcbc275def98ba61.js" async=""></script><script src="/cvgri.github.io/_next/static/chunks/619-9168df9c2a29b74b.js" async=""></script><script src="/cvgri.github.io/_next/static/chunks/app/research/page-ebbff567b58acdd6.js" async=""></script><title>CVGRI 2025</title><meta name="description" content="Developed by the CVGRI Team"/><link rel="icon" href="/cvgri.github.io/favicon.ico" type="image/x-icon" sizes="144x119"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"/><script src="/cvgri.github.io/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_728e81 __variable_c4d517"><div hidden=""><!--$--><!--/$--></div><nav class="navbar_navbar__FuZmF"><div class="navbar_container__qD6AK"><div class="navbar_logo__2CwkW"><img alt="CVGRI Logo" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" style="color:transparent" src="/image.png"/><a href="/cvgri.github.io#home">CVGRI</a></div><button class="navbar_hamburger__zVoYX">☰</button><ul class="navbar_navLinks__d4dCk "><li><a href="/cvgri.github.io#about">About</a></li><li><a href="/cvgri.github.io#research">Research</a></li><li><a href="/cvgri.github.io#students">Students</a></li><li><a href="/cvgri.github.io#program">Program</a></li><li><a href="/cvgri.github.io#staff">Staff</a></li><li><a href="/cvgri.github.io#gallery">Gallery</a></li></ul></div></nav><div class="page_page__ZU32B"><main><nav class="navbar_navbar__FuZmF"><div class="navbar_container__qD6AK"><div class="navbar_logo__2CwkW"><img alt="CVGRI Logo" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" style="color:transparent" src="/image.png"/><a href="/cvgri.github.io#home">CVGRI</a></div><button class="navbar_hamburger__zVoYX">☰</button><ul class="navbar_navLinks__d4dCk "><li><a href="/cvgri.github.io#about">About</a></li><li><a href="/cvgri.github.io#research">Research</a></li><li><a href="/cvgri.github.io#students">Students</a></li><li><a href="/cvgri.github.io#program">Program</a></li><li><a href="/cvgri.github.io#staff">Staff</a></li><li><a href="/cvgri.github.io#gallery">Gallery</a></li></ul></div></nav><section><section><div style="width:100%;height:720px;margin-top:65px;margin-bottom:20px;overflow:hidden;box-shadow:0 10px 30px rgba(0,0,0,0.1)"><img src="/Research Project Images/Diffusion Models in the Medical Field.png" alt="Research preview" style="width:100%;height:100%;object-fit:cover;transition:opacity 0.5s ease-in-out"/></div><section class="max-w-5xl mx-auto  py-12 px-6" style="padding:60px"><h2 class="text-4xl font-bold text-center text-cyan-700 mb-8  __className_0302e8" style="margin-bottom:30px;font-size:40px;text-align:center;color:#0e7490">Research Projects</h2><div class="space-y-10 "><div class="p-6"><br/><h3 class="text-xl font-semibold mb-3 __className_0302e8" style="color:#0e7490;font-size:25px;margin-bottom:15px">A Scoping Review on the Applications of Diffusion Models in the Medical Field</h3><br/><div class="text-base font-sans mb-3 __className_7b253d" style="color:#164044;font-size:17px">Given the recent advancements in computer vision, diffusion models have garnered significant attention within the field of medicine. This survey aims to provide a comprehensive overview of diffusion models in this domain, serving as a valuable resource for researchers navigating this rapidly evolving area.</div><br/><a href="/cvgri.github.io/research/group3"><button class="transition __className_0302e8 no-shadow" style="background-color:#A4E0E5;width:150px;height:44px;border-radius:31px;color:#3A8E9C;font-size:20px;box-shadow:none;border:none;margin-bottom:50px;transition:background color 0.3s ease;cursor:pointer">Read More</button></a></div><div class="p-6"><br/><h3 class="text-xl font-semibold mb-3 __className_0302e8" style="color:#0e7490;font-size:25px;margin-bottom:15px">Dopplegangers++: Improving 3D Reconstruction with Graph Algorithms for symmetric Objects</h3><br/><div class="text-base font-sans mb-3 __className_7b253d" style="color:#164044;font-size:17px">3D reconstruction has transformed dramatically with the rise of Structure-from-Motion (SfM) tools like COLMAP which leverage image collections to create detailed 3D models. Yet, these tools often stumble when faced with objects that appear visually similar from different perspectives, think symmetrical buildings, identical cups, or repeating textures.</div><br/><a href="/cvgri.github.io/research/group3"><button class="transition __className_0302e8 no-shadow" style="background-color:#A4E0E5;width:150px;height:44px;border-radius:31px;color:#3A8E9C;font-size:20px;box-shadow:none;border:none;margin-bottom:50px;transition:background color 0.3s ease;cursor:pointer">Read More</button></a></div><div class="p-6"><br/><h3 class="text-xl font-semibold mb-3 __className_0302e8" style="color:#0e7490;font-size:25px;margin-bottom:15px">Flash Stage: Relighting with Flashlight</h3><br/><div class="text-base font-sans mb-3 __className_7b253d" style="color:#164044;font-size:17px">The film and gaming industries have leveraged cutting-edge technologies for years to achieve stunning realism in digital characters and environments. One such tool is the Light Stage, a sophisticated setup used to capture how light interacts with a subject&#x27;s face from all directions. This data enables incredibly realistic relighting of faces in post-production, allowing characters to blend seamlessly into any scene. While the results are extraordinary, the Light Stage&#x27;s complexity and cost have largely confined it to high-budget productions in Hollywood. At FlashStage, we asked ourselves a simple question: What if this powerful technology could be reimagined in a way that’s affordable, accessible, and easy to use? Could we create a solution that brings professional-grade relighting capabilities into the hands of researchers, developers, and artists worldwide? The answer is a resounding yes!</div><br/><a href="/cvgri.github.io/research/group3"><button class="transition __className_0302e8 no-shadow" style="background-color:#A4E0E5;width:150px;height:44px;border-radius:31px;color:#3A8E9C;font-size:20px;box-shadow:none;border:none;margin-bottom:50px;transition:background color 0.3s ease;cursor:pointer">Read More</button></a></div><div class="p-6"><br/><h3 class="text-xl font-semibold mb-3 __className_0302e8" style="color:#0e7490;font-size:25px;margin-bottom:15px">Inconsistencies in Novel View Synthesis for Videos</h3><br/><div class="text-base font-sans mb-3 __className_7b253d" style="color:#164044;font-size:17px">In recent years, novel view synthesis has emerged as a groundbreaking area in the field of computer vision and AI. Novel view synthesis represents the generation of images and videos in a new perspective or point of view given an input image or video. NVS1 is a concept that has been studied for a while now; however, it was in the late 2010s and early 2020s that it was tackled with significant success due to advances made in machine learning. The technology holds big promise in various fields, such as realistic virtual environments, gaming, and filmmaking. In general, fields that relate to computer graphics are positively impacted by the boom of NVS. NVS has gone through significant changes since its inception. Engineers have solved a list of problems it had over the years; we will see some of these problems in other sections of this post. However, one issue that has yet to be solved in NVS is achieving video consistency throughout the frames of generated output videos. Video inconsistency is a broad term that we used to represent problems encountered in NVS-generated videos. The problems include changes in visual appearances of objects in the scene, unusual transitions in the scenes themselves, flickering backgrounds, inconsistent frames, and more. Solving video inconsistency in Novel View Synthesis is the core objective of our research. In this post, we will explore the phenomenon of video inconsistency, show how it is a major unsolved problem in the field of NVS, and suggest potential solutions to solve it.</div><br/><a href="/cvgri.github.io/research/group4"><button class="transition __className_0302e8 no-shadow" style="background-color:#A4E0E5;width:150px;height:44px;border-radius:31px;color:#3A8E9C;font-size:20px;box-shadow:none;border:none;margin-bottom:50px;transition:background color 0.3s ease;cursor:pointer">Read More</button></a></div><div class="p-6"><br/><h3 class="text-xl font-semibold mb-3 __className_0302e8" style="color:#0e7490;font-size:25px;margin-bottom:15px">Adversarial Pixels: Exploring Why Vision Tasks Aren’t Fully Solved</h3><br/><div class="text-base font-sans mb-3 __className_7b253d" style="color:#164044;font-size:17px">Current systems that utilize state-of-the art deep learning techniques have shown remarkable performance in vision and text tasks. We communicate with these systems and have made them part of our daily routines. Without a significant overlap in the semantics of our exchange, the communication would have been impossible. But when asking deeper questions (no pun intended) in how we humans and machines can extract the same meaning from an input or a query, we find that sometimes the same semantic information might not be extracted.</div><br/><a href="/cvgri.github.io/research/group5"><button class="transition __className_0302e8 no-shadow" style="background-color:#A4E0E5;width:150px;height:44px;border-radius:31px;color:#3A8E9C;font-size:20px;box-shadow:none;border:none;margin-bottom:50px;transition:background color 0.3s ease;cursor:pointer">Read More</button></a></div><div class="p-6"><br/><h3 class="text-xl font-semibold mb-3 __className_0302e8" style="color:#0e7490;font-size:25px;margin-bottom:15px">Language-Guided Image Segmentation Models: A Comparative Study of CLIPSeg, UniLSeg, and Lang_SAM</h3><br/><div class="text-base font-sans mb-3 __className_7b253d" style="color:#164044;font-size:17px">Image segmentation, the process of partitioning an image into meaningful regions, plays a vital role in computer vision applications such as autonomous vehicles, medical imaging, and content creation. Traditional segmentation methods, while effective in controlled environments, often rely on large labeled datasets and predefined object categories, making them less adaptable to new or unseen scenarios. The advent of vision-language models, particularly Contrastive Language-Image Pretraining (CLIP), has revolutionized this field. These models leverage a shared embedding space for text and images, enabling machines to interpret and respond to natural language prompts dynamically. This innovation empowers zero-shot and few-shot segmentation capabilities, where models can generalize across domains without extensive retraining or labeled data. Our work explores cutting-edge advancements in vision-language segmentation models, focusing on CLIP and its derivatives like CLIPSeg, LangSAM, and UniLSeg. We examine how these models align textual descriptions with visual features to deliver precise segmentation results across diverse applications. Through experiments and comparative analysis, we highlight their strengths, limitations, and potential to redefine how machines perceive and interact with visual data. This study serves as a window into the transformative potential of these technologies, pushing the boundaries of what is possible in automated image understanding. This post explores using CLIP in segmentation, focusing on its strengths, challenges, and future directions.</div><br/><a href="/cvgri.github.io/research/group3"><button class="transition __className_0302e8 no-shadow" style="background-color:#A4E0E5;width:150px;height:44px;border-radius:31px;color:#3A8E9C;font-size:20px;box-shadow:none;border:none;margin-bottom:50px;transition:background color 0.3s ease;cursor:pointer">Read More</button></a></div><div class="p-6"><br/><h3 class="text-xl font-semibold mb-3 __className_0302e8" style="color:#0e7490;font-size:25px;margin-bottom:15px">Automated Amharic Braille Recognition Using Image Processing and Contour Detection</h3><br/><div class="text-base font-sans mb-3 __className_7b253d" style="color:#164044;font-size:17px">Amharic Braille is essential for visually impaired individuals who read and write in Amharic. This project focuses on developing a computer vision-based solution to automatically detect and interpret Amharic Braille characters.</div><br/><a href="/cvgri.github.io/research/group3"><button class="transition __className_0302e8 no-shadow" style="background-color:#A4E0E5;width:150px;height:44px;border-radius:31px;color:#3A8E9C;font-size:20px;box-shadow:none;border:none;margin-bottom:50px;transition:background color 0.3s ease;cursor:pointer">Read More</button></a></div></div></section></section></section> </main><footer><footer class="footer_footer__WFt8a"><div class="footer_left__pBTxI"><img alt="CVGRI Logo" loading="lazy" width="60" height="50" decoding="async" data-nimg="1" style="color:transparent" src="/favicon.ico"/><span class="footer_logoText__6RK_H">CVGRI</span></div><div class="footer_center__LK0Ix"><p>Made by CVGRI Team</p><p class="footer_year__1VT3T">2025</p></div></footer></footer></div><!--$--><!--/$--><footer class="footer_footer__WFt8a"><div class="footer_left__pBTxI"><img alt="CVGRI Logo" loading="lazy" width="60" height="50" decoding="async" data-nimg="1" style="color:transparent" src="/favicon.ico"/><span class="footer_logoText__6RK_H">CVGRI</span></div><div class="footer_center__LK0Ix"><p>Made by CVGRI Team</p><p class="footer_year__1VT3T">2025</p></div></footer><script src="/cvgri.github.io/_next/static/chunks/webpack-fabe833255cd74ec.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[9766,[],\"\"]\n3:I[8924,[],\"\"]\n4:I[8605,[\"394\",\"static/chunks/394-dcbc275def98ba61.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"322\",\"static/chunks/app/research/page-ebbff567b58acdd6.js\"],\"default\"]\n5:I[1526,[\"394\",\"static/chunks/394-dcbc275def98ba61.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"322\",\"static/chunks/app/research/page-ebbff567b58acdd6.js\"],\"default\"]\n6:I[9645,[\"394\",\"static/chunks/394-dcbc275def98ba61.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"322\",\"static/chunks/app/research/page-ebbff567b58acdd6.js\"],\"default\"]\n7:I[4431,[],\"OutletBoundary\"]\n9:I[5278,[],\"AsyncMetadataOutlet\"]\nb:I[4431,[],\"ViewportBoundary\"]\nd:I[4431,[],\"MetadataBoundary\"]\ne:\"$Sreact.suspense\"\n10:I[7150,[],\"\"]\n:HL[\"/cvgri.github.io/_next/static/css/c83f8f8090aa4c07.css\",\"style\"]\n:HL[\"/cvgri.github.io/_next/static/css/322b103938224f59.css\",\"style\"]\n:HL[\"/cvgri.github.io/_next/static/css/48a2b02155d2754b.css\",\"style\"]\n:HL[\"/cvgri.github.io/_next/static/css/cdbf817f0b6ed54d.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"KeRCacRKoUFGG9nje-rBS\",\"p\":\"/cvgri.github.io\",\"c\":[\"\",\"research\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"research\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/cvgri.github.io/_next/static/css/c83f8f8090aa4c07.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css\"}]}],[\"$\",\"body\",null,{\"className\":\"__variable_728e81 __variable_c4d517\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]]}],{\"children\":[\"research\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/cvgri.github.io/_next/static/css/322b103938224f59.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[[\"$\",\"$L4\",null,{}],[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"$L5\",null,{}]]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"page_page__ZU32B\",\"children\":[[\"$\",\"main\",null,{\"children\":[[\"$\",\"$L4\",null,{}],[\"$\",\"section\",null,{\"children\":[\"$\",\"$L6\",null,{}]}],\" \"]}],[\"$\",\"footer\",null,{\"children\":[\"$\",\"$L5\",null,{}]}]]}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/cvgri.github.io/_next/static/css/48a2b02155d2754b.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/cvgri.github.io/_next/static/css/cdbf817f0b6ed54d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L7\",null,{\"children\":[\"$L8\",[\"$\",\"$L9\",null,{\"promise\":\"$@a\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],null],[\"$\",\"$Ld\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$e\",null,{\"fallback\":null,\"children\":\"$Lf\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$10\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n8:null\n"])</script><script>self.__next_f.push([1,"11:I[622,[],\"IconMark\"]\na:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"CVGRI 2025\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Developed by the CVGRI Team\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/cvgri.github.io/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"144x119\"}],[\"$\",\"$L11\",\"3\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"f:\"$a:metadata\"\n"])</script></body></html>