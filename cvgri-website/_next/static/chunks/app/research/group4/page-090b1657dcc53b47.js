(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[250],{2072:(e,s,a)=>{"use strict";a.d(s,{default:()=>h});var i=a(5155),t=a(8661),n=a.n(t),r=a(2115),o=a(5239),l=a(63),c=a(2497),f=a.n(c),d=a(8620);function h(){let e=(0,l.useRouter)(),s=d.e.group4,[a,t]=(0,r.useState)([]);return(0,r.useEffect)(()=>{t([...Object.values(d.e).filter(e=>e.slug!==s.slug)].sort(()=>Math.random()-.5).slice(0,2))},[s.slug]),(0,i.jsxs)("main",{className:"jsx-22a9728020ff6a57 "+"".concat(f().className," page"),children:[(0,i.jsxs)("aside",{className:"jsx-22a9728020ff6a57 actions",children:[(0,i.jsxs)("button",{onClick:()=>e.push("/research"),className:"jsx-22a9728020ff6a57 btn back",children:[(0,i.jsx)("span",{className:"jsx-22a9728020ff6a57 btn-text",children:"← Back"}),(0,i.jsx)("span",{className:"jsx-22a9728020ff6a57 btn-icon",children:"←"})]}),(0,i.jsxs)("button",{onClick:()=>window.scrollTo({top:0,behavior:"smooth"}),className:"jsx-22a9728020ff6a57 btn scroll",children:[(0,i.jsx)("span",{className:"jsx-22a9728020ff6a57 btn-text",children:"↑ Top"}),(0,i.jsx)("span",{className:"jsx-22a9728020ff6a57 btn-icon",children:"↑"})]}),(0,i.jsxs)("button",{onClick:()=>window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"}),className:"jsx-22a9728020ff6a57 btn scroll",children:[(0,i.jsx)("span",{className:"jsx-22a9728020ff6a57 btn-text",children:"↓ Bottom"}),(0,i.jsx)("span",{className:"jsx-22a9728020ff6a57 btn-icon",children:"↓"})]})]}),(0,i.jsxs)("article",{className:"jsx-22a9728020ff6a57 blog",children:[(0,i.jsxs)("div",{className:"jsx-22a9728020ff6a57 blog-container",children:[(0,i.jsxs)("header",{className:"jsx-22a9728020ff6a57 header",children:[(0,i.jsx)("h1",{className:"jsx-22a9728020ff6a57",children:s.title}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57 subtitle",children:s.subtitle}),(0,i.jsxs)("span",{className:"jsx-22a9728020ff6a57 slug",children:["/",s.slug]})]}),(0,i.jsx)("div",{className:"jsx-22a9728020ff6a57 hero",children:(0,i.jsx)(o.default,{src:s.image,alt:s.title,fill:!0,priority:!0})}),(0,i.jsxs)("section",{className:"jsx-22a9728020ff6a57 content",children:[(0,i.jsx)("h2",{className:"jsx-22a9728020ff6a57",children:"1. Introduction"}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"In recent years, novel view synthesis has emerged as a groundbreaking area in the field of computer vision and AI. Novel view synthesis represents the generation of images and videos in a new perspective or point of view given an input image or video. NVS1 is a concept that has been studied for a while now; however, it was in the late 2010s and early 2020s that it was tackled with significant success due to advances made in machine learning. The technology holds big promise in various fields, such as realistic virtual environments, gaming, and filmmaking. In general, fields that relate to computer graphics are positively impacted by the boom of NVS."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"NVS has gone through significant changes since its inception. Engineers have solved a list of problems it had over the years; we will see some of these problems in other sections of this post. However, one issue that has yet to be solved in NVS is achieving video consistency throughout the frames of generated output videos."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"Video inconsistency is a broad term that we used to represent problems encountered in NVS-generated videos. The problems include changes in visual appearances of objects in the scene, unusual transitions in the scenes themselves, flickering backgrounds, inconsistent frames, and more. Solving video inconsistency in Novel View Synthesis is the core objective of our research. In this post, we will explore the phenomenon of video inconsistency, show how it is a major unsolved problem in the field of NVS, and suggest potential solutions to solve it."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h2",{className:"jsx-22a9728020ff6a57",children:"2. Literature Review"}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"For this research, we studied two diffusion models that have tackled NVS before. Traditionally, NVS was heavily reliant on dense multi-view captures taken from a lot of angles, which were necessary for the models to have reliable outputs. However, this method has practical limitations in scenarios with sparse input images. Some of the early steps taken to overcome this challenge utilize a regression-based NVS2 method. They require extensive training and are also limited to specific domains like indoor scenes and object-focused input images. These limitations hinder the practicability of NVS and were also gaining low-fidelity results. Therefore, researchers and engineers came up with a better solution."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"Diffusion-based NVS have recently emerged as a robust alternative for handling NVS tasks. They use generative models to manage scenes that can work on sparse input images. A lot of models have tried to increase the accuracy of these diffusion-based models by introducing their specific fixes. During our research, we studied two of these models."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"2.1 ZeroNVS"}),(0,i.jsxs)("p",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("b",{className:"jsx-22a9728020ff6a57",children:"Zero-Shot 360-Degree View Synthesis from a Single Image Model3"})," ","is designed to generate realistic 360-degree views of a scene from a single input image. This image can be a real-world image from any environment. The model is built to handle scenarios that include images with multiple objects and diverse and intricate backgrounds. The model generates a series of realistic views of the scene from different angles. These views represent how the input scene would appear if observed from varying perspectives, ensuring a comprehensive understanding of its 3D structure."]}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"The model was trained on object-centric images that have real backgrounds instead of controlled indoor scenes, natural scenarios, and indoor/outdoor home tours. The camera perspective movement includes 3D rotation, 3D translation, and camera FoV. And the model addresses scale ambiguity using depth quantities. Then the diffusion model is distilled in NeRF4, optimizing a 3D representation to match the diffusion model's 2D scene."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"The model introduced zero-shot performance in NVS, generating new views from previously untrained images. It also managed to add background diversity and scale handling. The limitations of this model are pose control and various inconsistencies in occluded regions. The model introduces various artifacts in increasingly complicated scenes as well. Its performance is also poor, taking up a large amount of compute to run an inference."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"2.2 ViewCrafter"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"ViewCrafter is the second model we studied. This model is relatively recent and state-of-the-art when it comes to NVS. It was created to address some of the key problems that previous models such as ZeroNVS exhibited. The major factor that sets it apart from its predecessors is the inclusion of point cloud-based representations of the input image. They provide explicit 3D priors that enable better spatial and pose control. The model also utilizes video diffusion models trained on large-scale datasets for plausible and consistent video content generation."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"The iterative view synthesis strategy is the basis for how the model generates these new views. It iteratively synthesizes novel views and updates the point cloud to reveal occlusions and expand coverage. In addition to that, the camera pose control is robust, as the model predicts the optimal poses to maximize coverage and minimize artifacts."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"Some of its applications are real-time rendering. The model optimizes a 3D-GS representation from generated views. The diffusion model integrated also allows text-to-3D generation. The model addresses the occlusion limitation past models had due to the iterative process and point cloud inclusion."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"The model has its limitations. Primarily, generation of extreme views over large perspective angle changes remains challenging with limited 3D priors or distorted point clouds. It is also computationally expensive due to the denoising and computational overheads. Additionally, in scenes that exhibit relatively complex structure and various objects, the model generates inconsistencies and artifacts that were not in the input image."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h2",{className:"jsx-22a9728020ff6a57",children:"3. Materials and Methods"}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"3.1 Dataset Selection"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"We utilized a combination of datasets featuring both simple and complex scenes to evaluate the performance of novel view synthesis (NVS) models. The datasets included:"}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"Simple backgrounds:"})," static environments with a single object or minimal movement (e.g., landscapes or stationary interiors)."]}),(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"Complex scenes:"})," dynamic environments containing multiple interacting objects, varied lighting, and partial occlusion, including:",(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Human interactions in indoor settings"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Balloons of various colors in motion"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Birds in flight within natural scenes"})]})]})]}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"3.2 Models Evaluated"}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"Viewcrafter: "})," Selected for its recent advancements and explicit claims of achieving consistency in novel views."]}),(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"ZeroNVS: "})," Chosen for its focus on NVS in complex environments, despite prior reports of inconsistency issues."]})]}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"3.3 Experimental Setup"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"We conducted tests under the following scenarios:"}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"Image-to-Video:"})," Assessing frame-to-frame consistency when transitioning from a single input image to a synthesized video."]}),(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"Video-to-Video:"})," Evaluating multi-frame consistency for novel view synthesis when generating videos from video inputs."]})]}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"To test consistency, we applied specific view synthesis conditions, including rotations:"}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"Vertical rotation (X-axis):"})," 0\xb0"]}),(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"Horizontal rotation (Y-axis):"})," \xb110\xb0"]}),(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"Zoom in and out (Z-axis):"})," \xb140\xb0"]})]}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"Each frame was generated independently from a defined angle, without referencing previously generated frames. This approach was intended to minimize the effect of the randomness of the diffusion model, which operates on probability distributions. The assumption was that generating each frame independently from its corresponding temporal frame would reduce the probability of inconsistencies. However, this lack of contextual referencing resulted in significant consistency issues."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"3.4 Consistency Metrics"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"We employed the following measures to evaluate consistency:"}),(0,i.jsx)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"Visual inspection"})," for issues such as:",(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Orientation and size changes in objects"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Disappearance of key subjects"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Distortion of background elements, lighting, or facial features"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Creation of irrelevant artifacts"})]})]})}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"3.5 Future Work: Enhancing Conditional Consistency in Novel View Synthesis"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"We plan to refine the consistency of NVS models by implementing a conditional consistency mechanism that evaluates and rejects inconsistent outputs. This approach is designed to address the challenges identified in the current study, particularly the lack of contextual referencing between independently generated frames."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h2",{className:"jsx-22a9728020ff6a57",children:"4. Results"}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"4.1 Image-to-Video Evaluation"}),(0,i.jsx)("h4",{className:"jsx-22a9728020ff6a57",children:"Observed Inconsistencies:"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"Birds Example (Rotations: Vertical = 0\xb0, Horizontal = \xb110\xb0, Zoom = -40\xb0)"}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Birds behaving contrary to input prompts"}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"One bird feeding on leaves in unprompted frames"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Movement of birds in opposite directions without input"})]}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Disappearance of the right bird and unexpected landing of the left bird on a leaf in the final frame"})]}),(0,i.jsx)("div",{className:"jsx-22a9728020ff6a57 inlineImage",children:(0,i.jsx)(o.default,{src:"/Research Project Images/Group4/1.png",alt:"inconsistency example",fill:!0,style:{objectFit:"contain"}})}),(0,i.jsx)("div",{className:"jsx-22a9728020ff6a57 inlineImage",children:(0,i.jsx)(o.default,{src:"/Research Project Images/Group4/2.gif",alt:"inconsistency example",fill:!0,style:{objectFit:"contain"}})}),(0,i.jsx)("h4",{className:"jsx-22a9728020ff6a57",children:"Balloons Example (Rotations: Vertical = 0\xb0, Horizontal = \xb110\xb0, Zoom = \xb140\xb0)"}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Disappearance of balloons (red, pink, blue) and creation of artifacts (e.g., yellow and green)"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Inconsistent color changes and unexpected movements"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"New balloons appearing without input prompts"})]}),(0,i.jsx)("div",{className:"jsx-22a9728020ff6a57 inlineImage",children:(0,i.jsx)(o.default,{src:"/Research Project Images/Group4/3.png",alt:"inconsistency example",fill:!0,style:{objectFit:"contain"}})}),(0,i.jsx)("div",{className:"jsx-22a9728020ff6a57 inlineImage",children:(0,i.jsx)(o.default,{src:"/Research Project Images/Group4/4.png",alt:"inconsistency example",fill:!0,style:{objectFit:"contain"}})}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"4.2 Video-to-Video Evaluation"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"Human characters exhibited the following issues:"}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Incorrect orientation of subjects"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Disappearance of secondary characters in subsequent frames"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Addition of irrelevant objects such as cabinets and oven trays"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Distortion in facial features and inconsistent human sizes"})]}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"We selected three frames from a video to conduct this experiment."}),(0,i.jsx)("div",{className:"jsx-22a9728020ff6a57 inlineImage",children:(0,i.jsx)(o.default,{src:"/Research Project Images/Group4/5.png",alt:"inconsistency example",fill:!0,style:{objectFit:"contain"}})}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"4.3 Comparison of Models"}),(0,i.jsx)("h4",{className:"jsx-22a9728020ff6a57",children:"ViewCrafter"}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Strong performance in static scenes"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Struggles with occlusion and dynamic elements"})]}),(0,i.jsx)("h4",{className:"jsx-22a9728020ff6a57",children:"ZeroNVS"}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Handles complex backgrounds moderately well"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Introduces severe inconsistencies in video outputs"}),(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"Resource Usage:"})," Requires significant GPU and RAM resources, posing scalability challenges"]})]}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"4.4 Key Insights"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"Both models demonstrate significant limitations in maintaining consistency for dynamic, complex scenes."}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Object disappearance and unnatural movements"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Inconsistent lighting and distortion of scene objects"}),(0,i.jsx)("li",{className:"jsx-22a9728020ff6a57",children:"Need for improvements in NVS models for real-world video synthesis"})]}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"Both image-to-video and video-to-video evaluations revealed object disappearance, inconsistent lighting, and unexpected artifacts across frames."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h2",{className:"jsx-22a9728020ff6a57",children:"5. Analysis"}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"This project investigated the consistency and reliability of novel view synthesis (NVS) models under both image-to-video and video-to-video generation settings, with a particular focus on dynamic, real-world scenarios."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"The findings indicate that while current NVS models excel at generating visually appealing single frames, they lack robust mechanisms for preserving object identity, spatial coherence, and semantic consistency across time. Common failure modes included object disappearance, unnatural motion, inconsistent lighting, and structural distortions. These issues were amplified in dynamic, real-world scenes where temporal dependencies are essential."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"The decision to generate frames independently—though intended to reduce diffusion randomness—ultimately exposed a critical limitation: the absence of temporal conditioning leads to significant degradation in consistency. This suggests that future NVS systems must incorporate explicit temporal constraints or memory mechanisms to bridge the gap between per-frame visual realism and coherent video synthesis."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("p",{className:"jsx-22a9728020ff6a57",children:"Overall, the analysis underscores the need for improved NVS architectures that integrate temporal awareness, stronger object-level representations, and more efficient resource utilization to support reliable novel view synthesis in complex, real-world applications."}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsx)("h2",{className:"jsx-22a9728020ff6a57",children:"Glossary"}),(0,i.jsxs)("ul",{className:"jsx-22a9728020ff6a57 text-list",children:[(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"NVS"}),": Novel View Synthesis"]}),(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"NeRF"}),": Neural Radiance Fields"]}),(0,i.jsxs)("li",{className:"jsx-22a9728020ff6a57",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57",children:"ZeroNVS"}),": Zero-shot 360-degree synthesis"]})]}),(0,i.jsx)("br",{className:"jsx-22a9728020ff6a57"}),(0,i.jsxs)("h4",{className:"jsx-22a9728020ff6a57",children:["Firi Berhane:"," ",(0,i.jsx)("a",{href:"https://www.linkedin.com/in/firi-berhane-7b3a05232/",className:"jsx-22a9728020ff6a57 linkColor",children:"LinkedIn"})]}),(0,i.jsxs)("h4",{className:"jsx-22a9728020ff6a57",children:["Hana Kassie:"," ",(0,i.jsx)("a",{href:"https://www.linkedin.com/in/hana-kassie-19b326227/",target:"_blank",className:"jsx-22a9728020ff6a57 linkColor",children:"LinkedIn"})]}),(0,i.jsxs)("h4",{className:"jsx-22a9728020ff6a57",children:["Joshua Mekuriaw:"," ",(0,i.jsx)("a",{href:"https://www.linkedin.com/in/joshua-tadesse-aa8081225/",target:"_blank",className:"jsx-22a9728020ff6a57 linkColor",children:"LinkedIn"})]}),(0,i.jsxs)("h4",{className:"jsx-22a9728020ff6a57",children:["Kushal Kedia:"," ",(0,i.jsx)("a",{href:"https://www.linkedin.com/in/kushal-kedia/",target:"_blank",className:"jsx-22a9728020ff6a57 linkColor",children:"LinkedIn"})]})]})]}),(0,i.jsxs)("aside",{className:"jsx-22a9728020ff6a57 sidebar",children:[(0,i.jsx)("h3",{className:"jsx-22a9728020ff6a57",children:"Other Research"}),(0,i.jsxs)("div",{className:"jsx-22a9728020ff6a57 cards",children:[a.map(s=>(0,i.jsxs)("div",{onClick:()=>e.push("/research/".concat(s.slug)),className:"jsx-22a9728020ff6a57 card",children:[(0,i.jsx)("div",{className:"jsx-22a9728020ff6a57 cardImage",children:(0,i.jsx)(o.default,{src:s.image,alt:s.title,fill:!0})}),(0,i.jsxs)("div",{className:"jsx-22a9728020ff6a57 cardText",children:[(0,i.jsx)("strong",{className:"jsx-22a9728020ff6a57 cardTitle",children:s.title}),(0,i.jsx)("span",{className:"jsx-22a9728020ff6a57 cardSubtitle",children:s.subtitle})]})]},s.slug)),(0,i.jsx)("button",{onClick:()=>e.push("/research"),className:"jsx-22a9728020ff6a57 view-all-text-btn",children:"View All Researches →"})]})]})]}),(0,i.jsx)(n(),{id:"22a9728020ff6a57",children:".page.jsx-22a9728020ff6a57{background:#f8fafc;padding:80px 0 160px}.blog.jsx-22a9728020ff6a57{max-width:1600px;margin-left:18%;padding:0 40px;position:relative}.blog-container.jsx-22a9728020ff6a57{max-width:1040px;margin:0}.header.jsx-22a9728020ff6a57 h1.jsx-22a9728020ff6a57{font-size:48px;color:#000;text-align:left;margin-bottom:8px}.subtitle.jsx-22a9728020ff6a57{color:#0e7490;font-size:22px;text-align:left;font-weight:500}.slug.jsx-22a9728020ff6a57{display:block;font-size:14px;color:#000;text-align:left;margin-top:8px;margin-bottom:48px}.hero.jsx-22a9728020ff6a57{position:relative;height:560px;border-radius:20px;overflow:hidden;margin-bottom:60px;border:1px solid rgba(0,0,0,.05)}.content.jsx-22a9728020ff6a57{font-size:19px;line-height:1.85;color:#000}.high-contrast-bold.jsx-22a9728020ff6a57{color:#000;font-weight:700}.text-list.jsx-22a9728020ff6a57{margin:24px 0;padding-left:20px;color:#000}.text-list.jsx-22a9728020ff6a57 li.jsx-22a9728020ff6a57{margin-bottom:12px}.inlineImage.jsx-22a9728020ff6a57{position:relative;height:450px;margin:48px 0;border-radius:14px;overflow:hidden;border:1px solid rgba(0,0,0,.05)}.actions.jsx-22a9728020ff6a57{position:fixed;bottom:32px;left:32px;display:flex;flex-direction:column;gap:12px;z-index:100;transition:opacity.3s ease}.btn.jsx-22a9728020ff6a57{border-radius:8px;padding:12px 20px;font-size:15px;font-weight:600;cursor:pointer;border:none;display:flex;align-items:center;justify-content:center;transition:all.2s ease;box-shadow:0 4px 12px rgba(0,0,0,.1)}.btn-icon.jsx-22a9728020ff6a57{display:none;font-size:18px}.btn.back.jsx-22a9728020ff6a57{background:#000;color:#fff}.btn.scroll.jsx-22a9728020ff6a57{background:#0e7490;color:#fff}.sidebar.jsx-22a9728020ff6a57{position:fixed;top:50%;right:60px;transform:translatey(-50%);width:280px;z-index:20}.sidebar.jsx-22a9728020ff6a57 h3.jsx-22a9728020ff6a57{margin-bottom:20px;color:#000;font-size:20px}.cards.jsx-22a9728020ff6a57{display:flex;flex-direction:column;gap:24px}.card.jsx-22a9728020ff6a57{cursor:pointer;transition:transform.2s}.card.jsx-22a9728020ff6a57:hover{transform:translatey(-4px)}.cardImage.jsx-22a9728020ff6a57{width:100%;height:160px;position:relative;border-radius:12px;overflow:hidden;border:1px solid rgba(0,0,0,.1)}.cardTitle.jsx-22a9728020ff6a57{display:block;margin-top:12px;color:#0e7490;font-size:16px}.cardSubtitle.jsx-22a9728020ff6a57{display:block;color:#000;font-size:14px}.linkColor.jsx-22a9728020ff6a57{color:#006effff}.view-all-text-btn.jsx-22a9728020ff6a57{background:transparent;border:none;color:#0e7490;font-weight:600;font-size:15px;cursor:pointer;padding:10px 0;text-align:left;width:fit-content;transition:all.2s ease}.view-all-text-btn.jsx-22a9728020ff6a57:hover{color:#000;transform:translatex(5px)}@media(max-width:1500px)and (min-width:701px){.actions.jsx-22a9728020ff6a57{opacity:.3}.actions.jsx-22a9728020ff6a57:hover{opacity:1}}@media(max-width:1700px){.blog.jsx-22a9728020ff6a57{margin-left:auto;margin-right:auto;max-width:1040px}.sidebar.jsx-22a9728020ff6a57{position:relative;top:0;right:0;transform:none;width:100%;margin:100px 0 0;padding-top:40px;border-top:1px solid rgba(0,0,0,.1)}.cards.jsx-22a9728020ff6a57{flex-direction:row;flex-wrap:wrap;justify-content:flex-start;align-items:center}.card.jsx-22a9728020ff6a57{width:300px}.view-all-text-btn.jsx-22a9728020ff6a57{margin-left:20px;padding:0}}@media(max-width:700px){.actions.jsx-22a9728020ff6a57{bottom:24px;left:20px;gap:12px;opacity:.6}.btn.jsx-22a9728020ff6a57{width:42px;height:42px;padding:0;border-radius:50%}.btn-text.jsx-22a9728020ff6a57{display:none}.btn-icon.jsx-22a9728020ff6a57{display:block}.blog.jsx-22a9728020ff6a57{margin-left:0;padding:0 20px}.header.jsx-22a9728020ff6a57 h1.jsx-22a9728020ff6a57{font-size:32px;text-align:center}.subtitle.jsx-22a9728020ff6a57,.slug.jsx-22a9728020ff6a57{text-align:center}.hero.jsx-22a9728020ff6a57{height:280px}.view-all-text-btn.jsx-22a9728020ff6a57{margin-left:0;margin-top:10px}}"})]})}},2497:e=>{e.exports={style:{fontFamily:"'Roboto', 'Roboto Fallback'",fontStyle:"normal"},className:"__className_5dbd02"}},5247:(e,s,a)=>{Promise.resolve().then(a.bind(a,2072))},5688:(e,s,a)=>{"use strict";var i=a(5704);a(6340);var t=a(2115),n=function(e){return e&&"object"==typeof e&&"default"in e?e:{default:e}}(t),r=void 0!==i&&i.env&&!0,o=function(e){return"[object String]"===Object.prototype.toString.call(e)},l=function(){function e(e){var s=void 0===e?{}:e,a=s.name,i=void 0===a?"stylesheet":a,t=s.optimizeForSpeed,n=void 0===t?r:t;c(o(i),"`name` must be a string"),this._name=i,this._deletedRulePlaceholder="#"+i+"-deleted-rule____{}",c("boolean"==typeof n,"`optimizeForSpeed` must be a boolean"),this._optimizeForSpeed=n,this._serverSheet=void 0,this._tags=[],this._injected=!1,this._rulesCount=0;var l="undefined"!=typeof window&&document.querySelector('meta[property="csp-nonce"]');this._nonce=l?l.getAttribute("content"):null}var s,a=e.prototype;return a.setOptimizeForSpeed=function(e){c("boolean"==typeof e,"`setOptimizeForSpeed` accepts a boolean"),c(0===this._rulesCount,"optimizeForSpeed cannot be when rules have already been inserted"),this.flush(),this._optimizeForSpeed=e,this.inject()},a.isOptimizeForSpeed=function(){return this._optimizeForSpeed},a.inject=function(){var e=this;if(c(!this._injected,"sheet already injected"),this._injected=!0,"undefined"!=typeof window&&this._optimizeForSpeed){this._tags[0]=this.makeStyleTag(this._name),this._optimizeForSpeed="insertRule"in this.getSheet(),this._optimizeForSpeed||(r||console.warn("StyleSheet: optimizeForSpeed mode not supported falling back to standard mode."),this.flush(),this._injected=!0);return}this._serverSheet={cssRules:[],insertRule:function(s,a){return"number"==typeof a?e._serverSheet.cssRules[a]={cssText:s}:e._serverSheet.cssRules.push({cssText:s}),a},deleteRule:function(s){e._serverSheet.cssRules[s]=null}}},a.getSheetForTag=function(e){if(e.sheet)return e.sheet;for(var s=0;s<document.styleSheets.length;s++)if(document.styleSheets[s].ownerNode===e)return document.styleSheets[s]},a.getSheet=function(){return this.getSheetForTag(this._tags[this._tags.length-1])},a.insertRule=function(e,s){if(c(o(e),"`insertRule` accepts only strings"),"undefined"==typeof window)return"number"!=typeof s&&(s=this._serverSheet.cssRules.length),this._serverSheet.insertRule(e,s),this._rulesCount++;if(this._optimizeForSpeed){var a=this.getSheet();"number"!=typeof s&&(s=a.cssRules.length);try{a.insertRule(e,s)}catch(s){return r||console.warn("StyleSheet: illegal rule: \n\n"+e+"\n\nSee https://stackoverflow.com/q/20007992 for more info"),-1}}else{var i=this._tags[s];this._tags.push(this.makeStyleTag(this._name,e,i))}return this._rulesCount++},a.replaceRule=function(e,s){if(this._optimizeForSpeed||"undefined"==typeof window){var a="undefined"!=typeof window?this.getSheet():this._serverSheet;if(s.trim()||(s=this._deletedRulePlaceholder),!a.cssRules[e])return e;a.deleteRule(e);try{a.insertRule(s,e)}catch(i){r||console.warn("StyleSheet: illegal rule: \n\n"+s+"\n\nSee https://stackoverflow.com/q/20007992 for more info"),a.insertRule(this._deletedRulePlaceholder,e)}}else{var i=this._tags[e];c(i,"old rule at index `"+e+"` not found"),i.textContent=s}return e},a.deleteRule=function(e){if("undefined"==typeof window)return void this._serverSheet.deleteRule(e);if(this._optimizeForSpeed)this.replaceRule(e,"");else{var s=this._tags[e];c(s,"rule at index `"+e+"` not found"),s.parentNode.removeChild(s),this._tags[e]=null}},a.flush=function(){this._injected=!1,this._rulesCount=0,"undefined"!=typeof window?(this._tags.forEach(function(e){return e&&e.parentNode.removeChild(e)}),this._tags=[]):this._serverSheet.cssRules=[]},a.cssRules=function(){var e=this;return"undefined"==typeof window?this._serverSheet.cssRules:this._tags.reduce(function(s,a){return a?s=s.concat(Array.prototype.map.call(e.getSheetForTag(a).cssRules,function(s){return s.cssText===e._deletedRulePlaceholder?null:s})):s.push(null),s},[])},a.makeStyleTag=function(e,s,a){s&&c(o(s),"makeStyleTag accepts only strings as second parameter");var i=document.createElement("style");this._nonce&&i.setAttribute("nonce",this._nonce),i.type="text/css",i.setAttribute("data-"+e,""),s&&i.appendChild(document.createTextNode(s));var t=document.head||document.getElementsByTagName("head")[0];return a?t.insertBefore(i,a):t.appendChild(i),i},s=[{key:"length",get:function(){return this._rulesCount}}],function(e,s){for(var a=0;a<s.length;a++){var i=s[a];i.enumerable=i.enumerable||!1,i.configurable=!0,"value"in i&&(i.writable=!0),Object.defineProperty(e,i.key,i)}}(e.prototype,s),e}();function c(e,s){if(!e)throw Error("StyleSheet: "+s+".")}var f=function(e){for(var s=5381,a=e.length;a;)s=33*s^e.charCodeAt(--a);return s>>>0},d={};function h(e,s){if(!s)return"jsx-"+e;var a=String(s),i=e+a;return d[i]||(d[i]="jsx-"+f(e+"-"+a)),d[i]}function m(e,s){"undefined"==typeof window&&(s=s.replace(/\/style/gi,"\\/style"));var a=e+s;return d[a]||(d[a]=s.replace(/__jsx-style-dynamic-selector/g,e)),d[a]}var x=function(){function e(e){var s=void 0===e?{}:e,a=s.styleSheet,i=void 0===a?null:a,t=s.optimizeForSpeed,n=void 0!==t&&t;this._sheet=i||new l({name:"styled-jsx",optimizeForSpeed:n}),this._sheet.inject(),i&&"boolean"==typeof n&&(this._sheet.setOptimizeForSpeed(n),this._optimizeForSpeed=this._sheet.isOptimizeForSpeed()),this._fromServer=void 0,this._indices={},this._instancesCounts={}}var s=e.prototype;return s.add=function(e){var s=this;void 0===this._optimizeForSpeed&&(this._optimizeForSpeed=Array.isArray(e.children),this._sheet.setOptimizeForSpeed(this._optimizeForSpeed),this._optimizeForSpeed=this._sheet.isOptimizeForSpeed()),"undefined"==typeof window||this._fromServer||(this._fromServer=this.selectFromServer(),this._instancesCounts=Object.keys(this._fromServer).reduce(function(e,s){return e[s]=0,e},{}));var a=this.getIdAndRules(e),i=a.styleId,t=a.rules;if(i in this._instancesCounts){this._instancesCounts[i]+=1;return}var n=t.map(function(e){return s._sheet.insertRule(e)}).filter(function(e){return -1!==e});this._indices[i]=n,this._instancesCounts[i]=1},s.remove=function(e){var s=this,a=this.getIdAndRules(e).styleId;if(function(e,s){if(!e)throw Error("StyleSheetRegistry: "+s+".")}(a in this._instancesCounts,"styleId: `"+a+"` not found"),this._instancesCounts[a]-=1,this._instancesCounts[a]<1){var i=this._fromServer&&this._fromServer[a];i?(i.parentNode.removeChild(i),delete this._fromServer[a]):(this._indices[a].forEach(function(e){return s._sheet.deleteRule(e)}),delete this._indices[a]),delete this._instancesCounts[a]}},s.update=function(e,s){this.add(s),this.remove(e)},s.flush=function(){this._sheet.flush(),this._sheet.inject(),this._fromServer=void 0,this._indices={},this._instancesCounts={}},s.cssRules=function(){var e=this,s=this._fromServer?Object.keys(this._fromServer).map(function(s){return[s,e._fromServer[s]]}):[],a=this._sheet.cssRules();return s.concat(Object.keys(this._indices).map(function(s){return[s,e._indices[s].map(function(e){return a[e].cssText}).join(e._optimizeForSpeed?"":"\n")]}).filter(function(e){return!!e[1]}))},s.styles=function(e){var s,a;return s=this.cssRules(),void 0===(a=e)&&(a={}),s.map(function(e){var s=e[0],i=e[1];return n.default.createElement("style",{id:"__"+s,key:"__"+s,nonce:a.nonce?a.nonce:void 0,dangerouslySetInnerHTML:{__html:i}})})},s.getIdAndRules=function(e){var s=e.children,a=e.dynamic,i=e.id;if(a){var t=h(i,a);return{styleId:t,rules:Array.isArray(s)?s.map(function(e){return m(t,e)}):[m(t,s)]}}return{styleId:h(i),rules:Array.isArray(s)?s:[s]}},s.selectFromServer=function(){return Array.prototype.slice.call(document.querySelectorAll('[id^="__jsx-"]')).reduce(function(e,s){return e[s.id.slice(2)]=s,e},{})},e}(),u=t.createContext(null);u.displayName="StyleSheetContext";var p=n.default.useInsertionEffect||n.default.useLayoutEffect,j="undefined"!=typeof window?new x:void 0;function g(e){var s=j||t.useContext(u);return s&&("undefined"==typeof window?s.add(e):p(function(){return s.add(e),function(){s.remove(e)}},[e.id,String(e.dynamic)])),null}g.dynamic=function(e){return e.map(function(e){return h(e[0],e[1])}).join(" ")},s.style=g},6340:()=>{},8620:(e,s,a)=>{"use strict";a.d(s,{e:()=>i});let i={group1:{slug:"group1",title:"A Scoping Review on the Applications of Diffusion Models in the Medical Field",subtitle:"Naomi Eskinder Woudneh, Sebrina Akmel Mudesir, Zemzem Hibet Oumer, Sosna Worku Achamyeleh",image:"/Research Project Images/Diffusion Models in the Medical Field.png"},group2:{slug:"group2",title:"Dopplegangers++: Improving 3D Reconstruction with Graph Algorithms for symmetric Objects",subtitle:"Tinsae Tadesse, Milki Hasena, Gemmechu Hassena",image:"/Research Project Images/Adversarial Pixels.png"},group3:{slug:"group3",title:"Flash Stage: Relighting with Flashlight",subtitle:"Nahom Temesgen, Lealem Kinfe, Nanati Oumer, Haileyesus Kassahun, Surafel Mulaw, Gemmechu Hassena",image:"/Research Project Images/FlashStagePoster.png"},group4:{slug:"group4",title:"Inconsistencies in Novel View Synthesis for Videos",subtitle:"Firi Berhane, Hana Andargie, Joshua Tadesse, Kushal Kedia",image:"/Research Project Images/Inconsistencies in Novel View Synthesis for Videos.png"},group5:{slug:"group5",title:"Adversarial Pixels: Exploring Why Vision Tasks Aren’t Fully Solved",subtitle:"Dawit Getahun, Abenezer Kebede, Mahlet Dereje, Ayda Sultan",image:"/Research Project Images/Adversarial Pixels.png"},group6:{slug:"group6",title:"Language-Guided Image Segmentation Models: A Comparative Study of CLIPSeg, UniLSeg, and Lang_SAM",subtitle:"Mebatsion Sahle, Seifegebreal Mosisa, Alpha Lencho, Yared Solomon, Elias Girma, Mahlet Assefa, Ayda Sultan, Mahlet Dereje",image:"/Research Project Images/Adversarial Pixels.png"},group7:{slug:"group7",title:"Automated Amharic Braille Recognition Using Image Processing and Contour Detection",subtitle:"Amir Ahmedin, Amir Fasil, Tsehay Goremes, Henok Belayneh, Tamiru Alemnew, Surafel Kindu",image:"/Research Project Images/Adversarial Pixels.png"}}},8661:(e,s,a)=>{"use strict";e.exports=a(5688).style}},e=>{e.O(0,[529,394,441,255,358],()=>e(e.s=5247)),_N_E=e.O()}]);