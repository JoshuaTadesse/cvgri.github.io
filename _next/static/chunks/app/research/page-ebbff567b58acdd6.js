(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[322],{1526:(e,i,t)=>{"use strict";t.d(i,{default:()=>r});var a=t(5155),s=t(2185),n=t.n(s),o=t(5239);function r(){return(0,a.jsxs)("footer",{className:n().footer,children:[(0,a.jsxs)("div",{className:n().left,children:[(0,a.jsx)(o.default,{src:"/favicon.ico",alt:"CVGRI Logo",width:60,height:50}),(0,a.jsx)("span",{className:n().logoText,children:"CVGRI"})]}),(0,a.jsxs)("div",{className:n().center,children:[(0,a.jsx)("p",{children:"Made by CVGRI Team"}),(0,a.jsx)("p",{className:n().year,children:"2025"})]})]})}},1712:e=>{e.exports={navbar:"navbar_navbar__FuZmF",container:"navbar_container__qD6AK",logo:"navbar_logo__2CwkW",navLinks:"navbar_navLinks__d4dCk",hamburger:"navbar_hamburger__zVoYX",show:"navbar_show__mFtJQ"}},2185:e=>{e.exports={footer:"footer_footer__WFt8a",left:"footer_left__pBTxI",logoText:"footer_logoText__6RK_H",center:"footer_center__LK0Ix",year:"footer_year__1VT3T"}},2436:e=>{e.exports={page:"page_page__ZU32B",main:"page_main__GlU4n",logo:"page_logo__7fc9l",ctas:"page_ctas__g5wGe",primary:"page_primary__V8M9Y",secondary:"page_secondary__lm_PT",footer:"page_footer__sHKi3",hero:"page_hero__SKW6o",heroOverlay:"page_heroOverlay__M87tP",heroText:"page_heroText__g5S3T",hh1:"page_hh1__fHV81"}},7702:(e,i,t)=>{Promise.resolve().then(t.t.bind(t,2436,23)),Promise.resolve().then(t.bind(t,9645)),Promise.resolve().then(t.bind(t,1526)),Promise.resolve().then(t.bind(t,8605))},8421:e=>{e.exports={style:{fontFamily:"'Ubuntu Sans', 'Ubuntu Sans Fallback'",fontWeight:400,fontStyle:"normal"},className:"__className_7b253d"}},8605:(e,i,t)=>{"use strict";t.d(i,{default:()=>d});var a=t(5155),s=t(1712),n=t.n(s),o=t(2619),r=t.n(o),l=t(5239),c=t(2115),h=t(63);function d(){let[e,i]=(0,c.useState)(!1),t="/"===(0,h.usePathname)(),s=()=>{i(!1)};return(0,a.jsx)("nav",{className:n().navbar,children:(0,a.jsxs)("div",{className:n().container,children:[(0,a.jsxs)("div",{className:n().logo,children:[(0,a.jsx)(l.default,{src:"/image.png",alt:"CVGRI Logo",width:40,height:40}),(0,a.jsx)(r(),{href:t?"#home":"/#home",onClick:s,children:"CVGRI"})]}),(0,a.jsx)("button",{className:n().hamburger,onClick:()=>{i(!e)},children:"☰"}),(0,a.jsxs)("ul",{className:"".concat(n().navLinks," ").concat(e?n().show:""),children:[(0,a.jsx)("li",{children:(0,a.jsx)(r(),{href:t?"#about":"/#about",onClick:s,children:"About"})}),(0,a.jsx)("li",{children:(0,a.jsx)(r(),{href:t?"#research":"/#research",onClick:s,children:"Research"})}),(0,a.jsx)("li",{children:(0,a.jsx)(r(),{href:t?"#students":"/#students",onClick:s,children:"Students"})}),(0,a.jsx)("li",{children:(0,a.jsx)(r(),{href:t?"#program":"/#program",onClick:s,children:"Program"})}),(0,a.jsx)("li",{children:(0,a.jsx)(r(),{href:t?"#staff":"/#staff",onClick:s,children:"Staff"})}),(0,a.jsx)("li",{children:(0,a.jsx)(r(),{href:t?"#gallery":"/#gallery",onClick:s,children:"Gallery"})})]})]})})}},8998:e=>{e.exports={style:{fontFamily:"'Monomaniac One', 'Monomaniac One Fallback'",fontWeight:400,fontStyle:"normal"},className:"__className_0302e8"}},9645:(e,i,t)=>{"use strict";t.d(i,{default:()=>g});var a=t(5155),s=t(8998),n=t.n(s),o=t(8421),r=t.n(o),l=t(2115),c=t(2619),h=t.n(c);let d=[{title:"A Scoping Review on the Applications of Diffusion Models in the Medical Field",description:"Given the recent advancements in computer vision, diffusion models have garnered significant attention within the field of medicine. This survey aims to provide a comprehensive overview of diffusion models in this domain, serving as a valuable resource for researchers navigating this rapidly evolving area.",link:"/research/group3",image:"/Research Project Images/Diffusion Models in the Medical Field.png"},{title:"Dopplegangers++: Improving 3D Reconstruction with Graph Algorithms for symmetric Objects",description:"3D reconstruction has transformed dramatically with the rise of Structure-from-Motion (SfM) tools like COLMAP which leverage image collections to create detailed 3D models. Yet, these tools often stumble when faced with objects that appear visually similar from different perspectives, think symmetrical buildings, identical cups, or repeating textures.",link:"/research/group3",image:"/Research Project Images/Dopplegangers.png"},{title:"Flash Stage: Relighting with Flashlight",description:"The film and gaming industries have leveraged cutting-edge technologies for years to achieve stunning realism in digital characters and environments. One such tool is the Light Stage, a sophisticated setup used to capture how light interacts with a subject's face from all directions. This data enables incredibly realistic relighting of faces in post-production, allowing characters to blend seamlessly into any scene. While the results are extraordinary, the Light Stage's complexity and cost have largely confined it to high-budget productions in Hollywood. At FlashStage, we asked ourselves a simple question: What if this powerful technology could be reimagined in a way that’s affordable, accessible, and easy to use? Could we create a solution that brings professional-grade relighting capabilities into the hands of researchers, developers, and artists worldwide? The answer is a resounding yes!",link:"/research/group3",image:"/Research Project Images/FlashStagePoster.png"},{title:"Inconsistencies in Novel View Synthesis for Videos",description:"In recent years, novel view synthesis has emerged as a groundbreaking area in the field of computer vision and AI. Novel view synthesis represents the generation of images and videos in a new perspective or point of view given an input image or video. NVS1 is a concept that has been studied for a while now; however, it was in the late 2010s and early 2020s that it was tackled with significant success due to advances made in machine learning. The technology holds big promise in various fields, such as realistic virtual environments, gaming, and filmmaking. In general, fields that relate to computer graphics are positively impacted by the boom of NVS. NVS has gone through significant changes since its inception. Engineers have solved a list of problems it had over the years; we will see some of these problems in other sections of this post. However, one issue that has yet to be solved in NVS is achieving video consistency throughout the frames of generated output videos. Video inconsistency is a broad term that we used to represent problems encountered in NVS-generated videos. The problems include changes in visual appearances of objects in the scene, unusual transitions in the scenes themselves, flickering backgrounds, inconsistent frames, and more. Solving video inconsistency in Novel View Synthesis is the core objective of our research. In this post, we will explore the phenomenon of video inconsistency, show how it is a major unsolved problem in the field of NVS, and suggest potential solutions to solve it.",link:"/research/group4",image:"/Research Project Images/Inconsistencies in Novel View Synthesis for Videos.png"},{title:"Adversarial Pixels: Exploring Why Vision Tasks Aren’t Fully Solved",description:"Current systems that utilize state-of-the art deep learning techniques have shown remarkable performance in vision and text tasks. We communicate with these systems and have made them part of our daily routines. Without a significant overlap in the semantics of our exchange, the communication would have been impossible. But when asking deeper questions (no pun intended) in how we humans and machines can extract the same meaning from an input or a query, we find that sometimes the same semantic information might not be extracted.",link:"/research/group5",image:"/Research Project Images/Adversarial Pixels.png"},{title:"Language-Guided Image Segmentation Models: A Comparative Study of CLIPSeg, UniLSeg, and Lang_SAM",description:"Image segmentation, the process of partitioning an image into meaningful regions, plays a vital role in computer vision applications such as autonomous vehicles, medical imaging, and content creation. Traditional segmentation methods, while effective in controlled environments, often rely on large labeled datasets and predefined object categories, making them less adaptable to new or unseen scenarios. The advent of vision-language models, particularly Contrastive Language-Image Pretraining (CLIP), has revolutionized this field. These models leverage a shared embedding space for text and images, enabling machines to interpret and respond to natural language prompts dynamically. This innovation empowers zero-shot and few-shot segmentation capabilities, where models can generalize across domains without extensive retraining or labeled data. Our work explores cutting-edge advancements in vision-language segmentation models, focusing on CLIP and its derivatives like CLIPSeg, LangSAM, and UniLSeg. We examine how these models align textual descriptions with visual features to deliver precise segmentation results across diverse applications. Through experiments and comparative analysis, we highlight their strengths, limitations, and potential to redefine how machines perceive and interact with visual data. This study serves as a window into the transformative potential of these technologies, pushing the boundaries of what is possible in automated image understanding. This post explores using CLIP in segmentation, focusing on its strengths, challenges, and future directions.",link:"/research/group3",image:"/Research Project Images/Language-Guided Image Segmentation Models.png"},{title:"Automated Amharic Braille Recognition Using Image Processing and Contour Detection",description:"Amharic Braille is essential for visually impaired individuals who read and write in Amharic. This project focuses on developing a computer vision-based solution to automatically detect and interpret Amharic Braille characters.",link:"/research/group3",image:"/Research Project Images/Automated_Amharic_Braille_Recognition_Using_Image_Processing_and.png"}],g=()=>{let[e,i]=(0,l.useState)(0);return(0,l.useEffect)(()=>{let e=setInterval(()=>{i(e=>e===d.length-1?0:e+1)},3e3);return()=>clearInterval(e)},[]),(0,a.jsxs)("section",{children:[(0,a.jsx)("div",{style:{width:"100%",height:"720px",marginTop:"65px",marginBottom:"20px",overflow:"hidden",boxShadow:"0 10px 30px rgba(0,0,0,0.1)"},children:(0,a.jsx)("img",{src:d[e].image,alt:"Research preview",style:{width:"100%",height:"100%",objectFit:"cover",transition:"opacity 0.5s ease-in-out"}})}),(0,a.jsxs)("section",{className:"max-w-5xl mx-auto  py-12 px-6",style:{padding:"60px"},children:[(0,a.jsx)("h2",{className:"text-4xl font-bold text-center text-cyan-700 mb-8  ".concat(n().className),style:{marginBottom:"30px",fontSize:"40px",textAlign:"center",color:"#0e7490"},children:"Research Projects"}),(0,a.jsx)("div",{className:"space-y-10 ",children:d.map((e,i)=>(0,a.jsxs)("div",{className:"p-6",children:[(0,a.jsx)("br",{}),(0,a.jsx)("h3",{className:"text-xl font-semibold mb-3 ".concat(n().className),style:{color:"#0e7490",fontSize:"25px",marginBottom:"15px"},children:e.title}),(0,a.jsx)("br",{}),(0,a.jsx)("div",{className:"text-base font-sans mb-3 ".concat(r().className),style:{color:"#164044",fontSize:"17px"},children:e.description}),(0,a.jsx)("br",{}),(0,a.jsx)(h(),{href:e.link,children:(0,a.jsx)("button",{className:"transition ".concat(n().className," no-shadow"),style:{backgroundColor:"#A4E0E5",width:"150px",height:"44px",borderRadius:"31px",color:"#3A8E9C",fontSize:"20px",boxShadow:"none",border:"none",marginBottom:"50px",transition:"background color 0.3s ease",cursor:"pointer"},onMouseOver:e=>(e.currentTarget.style.backgroundColor="#0e7490",e.currentTarget.style.color="#A4E0E5"),onMouseOut:e=>(e.currentTarget.style.backgroundColor="#A4E0E5",e.currentTarget.style.color="#3A8E9C"),children:"Read More"})})]},i))})]})]})}}},e=>{e.O(0,[135,500,927,394,619,441,255,358],()=>e(e.s=7702)),_N_E=e.O()}]);