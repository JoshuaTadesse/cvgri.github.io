<!DOCTYPE html><!--xR7L3hmiRj1pOA2XqAz5n--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/Research Project Images/Inconsistencies in Novel View Synthesis for Videos.png"/><link rel="stylesheet" href="/cvgri.github.io/_next/static/css/c83f8f8090aa4c07.css" data-precedence="next"/><link rel="stylesheet" href="/cvgri.github.io/_next/static/css/322b103938224f59.css" data-precedence="next"/><link rel="stylesheet" href="/cvgri.github.io/_next/static/css/f71f4c7c3128a037.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/cvgri.github.io/_next/static/chunks/webpack-cbc3379284357474.js"/><script src="/cvgri.github.io/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/cvgri.github.io/_next/static/chunks/255-0aa33e93919bb743.js" async=""></script><script src="/cvgri.github.io/_next/static/chunks/main-app-c40da99ebe4848d2.js" async=""></script><script src="/cvgri.github.io/_next/static/chunks/394-6af1e34207d32b87.js" async=""></script><script src="/cvgri.github.io/_next/static/chunks/619-9168df9c2a29b74b.js" async=""></script><script src="/cvgri.github.io/_next/static/chunks/app/research/page-ebbff567b58acdd6.js" async=""></script><script src="/cvgri.github.io/_next/static/chunks/app/research/group4/page-090b1657dcc53b47.js" async=""></script><title>CVGRI 2025</title><meta name="description" content="Developed by the CVGRI Team"/><link rel="icon" href="/cvgri.github.io/favicon.ico" type="image/x-icon" sizes="144x119"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"/><script src="/cvgri.github.io/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_728e81 __variable_c4d517"><div hidden=""><!--$--><!--/$--></div><nav class="navbar_navbar__FuZmF"><div class="navbar_container__qD6AK"><div class="navbar_logo__2CwkW"><img alt="CVGRI Logo" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" style="color:transparent" src="/image.png"/><a href="/cvgri.github.io#home">CVGRI</a></div><button class="navbar_hamburger__zVoYX">☰</button><ul class="navbar_navLinks__d4dCk "><li><a href="/cvgri.github.io#about">About</a></li><li><a href="/cvgri.github.io#research">Research</a></li><li><a href="/cvgri.github.io#students">Students</a></li><li><a href="/cvgri.github.io#program">Program</a></li><li><a href="/cvgri.github.io#staff">Staff</a></li><li><a href="/cvgri.github.io#gallery">Gallery</a></li></ul></div></nav><main class="jsx-22a9728020ff6a57 __className_5dbd02 page"><aside class="jsx-22a9728020ff6a57 actions"><button class="jsx-22a9728020ff6a57 btn back"><span class="jsx-22a9728020ff6a57 btn-text">← Back</span><span class="jsx-22a9728020ff6a57 btn-icon">←</span></button><button class="jsx-22a9728020ff6a57 btn scroll"><span class="jsx-22a9728020ff6a57 btn-text">↑ Top</span><span class="jsx-22a9728020ff6a57 btn-icon">↑</span></button><button class="jsx-22a9728020ff6a57 btn scroll"><span class="jsx-22a9728020ff6a57 btn-text">↓ Bottom</span><span class="jsx-22a9728020ff6a57 btn-icon">↓</span></button></aside><article class="jsx-22a9728020ff6a57 blog"><div class="jsx-22a9728020ff6a57 blog-container"><header class="jsx-22a9728020ff6a57 header"><h1 class="jsx-22a9728020ff6a57">Inconsistencies in Novel View Synthesis for Videos</h1><p class="jsx-22a9728020ff6a57 subtitle">Firi Berhane, Hana Andargie, Joshua Tadesse, Kushal Kedia</p><span class="jsx-22a9728020ff6a57 slug">/<!-- -->group4</span></header><div class="jsx-22a9728020ff6a57 hero"><img alt="Inconsistencies in Novel View Synthesis for Videos" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/Research Project Images/Inconsistencies in Novel View Synthesis for Videos.png"/></div><section class="jsx-22a9728020ff6a57 content"><h2 class="jsx-22a9728020ff6a57">1. Introduction</h2><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">In recent years, novel view synthesis has emerged as a groundbreaking area in the field of computer vision and AI. Novel view synthesis represents the generation of images and videos in a new perspective or point of view given an input image or video. NVS1 is a concept that has been studied for a while now; however, it was in the late 2010s and early 2020s that it was tackled with significant success due to advances made in machine learning. The technology holds big promise in various fields, such as realistic virtual environments, gaming, and filmmaking. In general, fields that relate to computer graphics are positively impacted by the boom of NVS.</p><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">NVS has gone through significant changes since its inception. Engineers have solved a list of problems it had over the years; we will see some of these problems in other sections of this post. However, one issue that has yet to be solved in NVS is achieving video consistency throughout the frames of generated output videos.</p><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">Video inconsistency is a broad term that we used to represent problems encountered in NVS-generated videos. The problems include changes in visual appearances of objects in the scene, unusual transitions in the scenes themselves, flickering backgrounds, inconsistent frames, and more. Solving video inconsistency in Novel View Synthesis is the core objective of our research. In this post, we will explore the phenomenon of video inconsistency, show how it is a major unsolved problem in the field of NVS, and suggest potential solutions to solve it.</p><br class="jsx-22a9728020ff6a57"/><br class="jsx-22a9728020ff6a57"/><h2 class="jsx-22a9728020ff6a57">2. Literature Review</h2><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">For this research, we studied two diffusion models that have tackled NVS before. Traditionally, NVS was heavily reliant on dense multi-view captures taken from a lot of angles, which were necessary for the models to have reliable outputs. However, this method has practical limitations in scenarios with sparse input images. Some of the early steps taken to overcome this challenge utilize a regression-based NVS2 method. They require extensive training and are also limited to specific domains like indoor scenes and object-focused input images. These limitations hinder the practicability of NVS and were also gaining low-fidelity results. Therefore, researchers and engineers came up with a better solution.</p><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">Diffusion-based NVS have recently emerged as a robust alternative for handling NVS tasks. They use generative models to manage scenes that can work on sparse input images. A lot of models have tried to increase the accuracy of these diffusion-based models by introducing their specific fixes. During our research, we studied two of these models.</p><br class="jsx-22a9728020ff6a57"/><h3 class="jsx-22a9728020ff6a57">2.1 ZeroNVS</h3><p class="jsx-22a9728020ff6a57"><b class="jsx-22a9728020ff6a57">Zero-Shot 360-Degree View Synthesis from a Single Image Model3</b> <!-- -->is designed to generate realistic 360-degree views of a scene from a single input image. This image can be a real-world image from any environment. The model is built to handle scenarios that include images with multiple objects and diverse and intricate backgrounds. The model generates a series of realistic views of the scene from different angles. These views represent how the input scene would appear if observed from varying perspectives, ensuring a comprehensive understanding of its 3D structure.</p><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">The model was trained on object-centric images that have real backgrounds instead of controlled indoor scenes, natural scenarios, and indoor/outdoor home tours. The camera perspective movement includes 3D rotation, 3D translation, and camera FoV. And the model addresses scale ambiguity using depth quantities. Then the diffusion model is distilled in NeRF4, optimizing a 3D representation to match the diffusion model&#x27;s 2D scene.</p><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">The model introduced zero-shot performance in NVS, generating new views from previously untrained images. It also managed to add background diversity and scale handling. The limitations of this model are pose control and various inconsistencies in occluded regions. The model introduces various artifacts in increasingly complicated scenes as well. Its performance is also poor, taking up a large amount of compute to run an inference.</p><br class="jsx-22a9728020ff6a57"/><h3 class="jsx-22a9728020ff6a57">2.2 ViewCrafter</h3><p class="jsx-22a9728020ff6a57">ViewCrafter is the second model we studied. This model is relatively recent and state-of-the-art when it comes to NVS. It was created to address some of the key problems that previous models such as ZeroNVS exhibited. The major factor that sets it apart from its predecessors is the inclusion of point cloud-based representations of the input image. They provide explicit 3D priors that enable better spatial and pose control. The model also utilizes video diffusion models trained on large-scale datasets for plausible and consistent video content generation.</p><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">The iterative view synthesis strategy is the basis for how the model generates these new views. It iteratively synthesizes novel views and updates the point cloud to reveal occlusions and expand coverage. In addition to that, the camera pose control is robust, as the model predicts the optimal poses to maximize coverage and minimize artifacts.</p><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">Some of its applications are real-time rendering. The model optimizes a 3D-GS representation from generated views. The diffusion model integrated also allows text-to-3D generation. The model addresses the occlusion limitation past models had due to the iterative process and point cloud inclusion.</p><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">The model has its limitations. Primarily, generation of extreme views over large perspective angle changes remains challenging with limited 3D priors or distorted point clouds. It is also computationally expensive due to the denoising and computational overheads. Additionally, in scenes that exhibit relatively complex structure and various objects, the model generates inconsistencies and artifacts that were not in the input image.</p><br class="jsx-22a9728020ff6a57"/><br class="jsx-22a9728020ff6a57"/><h2 class="jsx-22a9728020ff6a57">3. Materials and Methods</h2><br class="jsx-22a9728020ff6a57"/><h3 class="jsx-22a9728020ff6a57">3.1 Dataset Selection</h3><p class="jsx-22a9728020ff6a57">We utilized a combination of datasets featuring both simple and complex scenes to evaluate the performance of novel view synthesis (NVS) models. The datasets included:</p><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">Simple backgrounds:</strong> static environments with a single object or minimal movement (e.g., landscapes or stationary interiors).</li><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">Complex scenes:</strong> dynamic environments containing multiple interacting objects, varied lighting, and partial occlusion, including:<ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57">Human interactions in indoor settings</li><li class="jsx-22a9728020ff6a57">Balloons of various colors in motion</li><li class="jsx-22a9728020ff6a57">Birds in flight within natural scenes</li></ul></li></ul><br class="jsx-22a9728020ff6a57"/><h3 class="jsx-22a9728020ff6a57">3.2 Models Evaluated</h3><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">Viewcrafter: </strong> Selected for its recent advancements and explicit claims of achieving consistency in novel views.</li><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">ZeroNVS: </strong> Chosen for its focus on NVS in complex environments, despite prior reports of inconsistency issues.</li></ul><br class="jsx-22a9728020ff6a57"/><h3 class="jsx-22a9728020ff6a57">3.3 Experimental Setup</h3><p class="jsx-22a9728020ff6a57">We conducted tests under the following scenarios:</p><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">Image-to-Video:</strong> Assessing frame-to-frame consistency when transitioning from a single input image to a synthesized video.</li><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">Video-to-Video:</strong> Evaluating multi-frame consistency for novel view synthesis when generating videos from video inputs.</li></ul><p class="jsx-22a9728020ff6a57">To test consistency, we applied specific view synthesis conditions, including rotations:</p><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">Vertical rotation (X-axis):</strong> 0°</li><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">Horizontal rotation (Y-axis):</strong> ±10°</li><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">Zoom in and out (Z-axis):</strong> ±40°</li></ul><p class="jsx-22a9728020ff6a57">Each frame was generated independently from a defined angle, without referencing previously generated frames. This approach was intended to minimize the effect of the randomness of the diffusion model, which operates on probability distributions. The assumption was that generating each frame independently from its corresponding temporal frame would reduce the probability of inconsistencies. However, this lack of contextual referencing resulted in significant consistency issues.</p><br class="jsx-22a9728020ff6a57"/><h3 class="jsx-22a9728020ff6a57">3.4 Consistency Metrics</h3><p class="jsx-22a9728020ff6a57">We employed the following measures to evaluate consistency:</p><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">Visual inspection</strong> for issues such as:<ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57">Orientation and size changes in objects</li><li class="jsx-22a9728020ff6a57">Disappearance of key subjects</li><li class="jsx-22a9728020ff6a57">Distortion of background elements, lighting, or facial features</li><li class="jsx-22a9728020ff6a57">Creation of irrelevant artifacts</li></ul></li></ul><br class="jsx-22a9728020ff6a57"/><h3 class="jsx-22a9728020ff6a57">3.5 Future Work: Enhancing Conditional Consistency in Novel View Synthesis</h3><p class="jsx-22a9728020ff6a57">We plan to refine the consistency of NVS models by implementing a conditional consistency mechanism that evaluates and rejects inconsistent outputs. This approach is designed to address the challenges identified in the current study, particularly the lack of contextual referencing between independently generated frames.</p><br class="jsx-22a9728020ff6a57"/><br class="jsx-22a9728020ff6a57"/><h2 class="jsx-22a9728020ff6a57">4. Results</h2><br class="jsx-22a9728020ff6a57"/><h3 class="jsx-22a9728020ff6a57">4.1 Image-to-Video Evaluation</h3><h4 class="jsx-22a9728020ff6a57">Observed Inconsistencies:</h4><p class="jsx-22a9728020ff6a57">Birds Example (Rotations: Vertical = 0°, Horizontal = ±10°, Zoom = -40°)</p><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57">Birds behaving contrary to input prompts</li><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57">One bird feeding on leaves in unprompted frames</li><li class="jsx-22a9728020ff6a57">Movement of birds in opposite directions without input</li></ul><li class="jsx-22a9728020ff6a57">Disappearance of the right bird and unexpected landing of the left bird on a leaf in the final frame</li></ul><div class="jsx-22a9728020ff6a57 inlineImage"><img alt="inconsistency example" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;color:transparent" src="/Research Project Images/Group4/1.png"/></div><div class="jsx-22a9728020ff6a57 inlineImage"><img alt="inconsistency example" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;color:transparent" src="/Research Project Images/Group4/2.gif"/></div><h4 class="jsx-22a9728020ff6a57">Balloons Example (Rotations: Vertical = 0°, Horizontal = ±10°, Zoom = ±40°)</h4><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57">Disappearance of balloons (red, pink, blue) and creation of artifacts (e.g., yellow and green)</li><li class="jsx-22a9728020ff6a57">Inconsistent color changes and unexpected movements</li><li class="jsx-22a9728020ff6a57">New balloons appearing without input prompts</li></ul><div class="jsx-22a9728020ff6a57 inlineImage"><img alt="inconsistency example" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;color:transparent" src="/Research Project Images/Group4/3.png"/></div><div class="jsx-22a9728020ff6a57 inlineImage"><img alt="inconsistency example" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;color:transparent" src="/Research Project Images/Group4/4.png"/></div><br class="jsx-22a9728020ff6a57"/><h3 class="jsx-22a9728020ff6a57">4.2 Video-to-Video Evaluation</h3><p class="jsx-22a9728020ff6a57">Human characters exhibited the following issues:</p><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57">Incorrect orientation of subjects</li><li class="jsx-22a9728020ff6a57">Disappearance of secondary characters in subsequent frames</li><li class="jsx-22a9728020ff6a57">Addition of irrelevant objects such as cabinets and oven trays</li><li class="jsx-22a9728020ff6a57">Distortion in facial features and inconsistent human sizes</li></ul><p class="jsx-22a9728020ff6a57">We selected three frames from a video to conduct this experiment.</p><div class="jsx-22a9728020ff6a57 inlineImage"><img alt="inconsistency example" loading="lazy" decoding="async" data-nimg="fill" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:contain;color:transparent" src="/Research Project Images/Group4/5.png"/></div><br class="jsx-22a9728020ff6a57"/><h3 class="jsx-22a9728020ff6a57">4.3 Comparison of Models</h3><h4 class="jsx-22a9728020ff6a57">ViewCrafter</h4><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57">Strong performance in static scenes</li><li class="jsx-22a9728020ff6a57">Struggles with occlusion and dynamic elements</li></ul><h4 class="jsx-22a9728020ff6a57">ZeroNVS</h4><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57">Handles complex backgrounds moderately well</li><li class="jsx-22a9728020ff6a57">Introduces severe inconsistencies in video outputs</li><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">Resource Usage:</strong> Requires significant GPU and RAM resources, posing scalability challenges</li></ul><br class="jsx-22a9728020ff6a57"/><h3 class="jsx-22a9728020ff6a57">4.4 Key Insights</h3><p class="jsx-22a9728020ff6a57">Both models demonstrate significant limitations in maintaining consistency for dynamic, complex scenes.</p><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57">Object disappearance and unnatural movements</li><li class="jsx-22a9728020ff6a57">Inconsistent lighting and distortion of scene objects</li><li class="jsx-22a9728020ff6a57">Need for improvements in NVS models for real-world video synthesis</li></ul><p class="jsx-22a9728020ff6a57">Both image-to-video and video-to-video evaluations revealed object disappearance, inconsistent lighting, and unexpected artifacts across frames.</p><br class="jsx-22a9728020ff6a57"/><br class="jsx-22a9728020ff6a57"/><h2 class="jsx-22a9728020ff6a57">5. Analysis</h2><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">This project investigated the consistency and reliability of novel view synthesis (NVS) models under both image-to-video and video-to-video generation settings, with a particular focus on dynamic, real-world scenarios.</p><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">The findings indicate that while current NVS models excel at generating visually appealing single frames, they lack robust mechanisms for preserving object identity, spatial coherence, and semantic consistency across time. Common failure modes included object disappearance, unnatural motion, inconsistent lighting, and structural distortions. These issues were amplified in dynamic, real-world scenes where temporal dependencies are essential.</p><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">The decision to generate frames independently—though intended to reduce diffusion randomness—ultimately exposed a critical limitation: the absence of temporal conditioning leads to significant degradation in consistency. This suggests that future NVS systems must incorporate explicit temporal constraints or memory mechanisms to bridge the gap between per-frame visual realism and coherent video synthesis.</p><br class="jsx-22a9728020ff6a57"/><p class="jsx-22a9728020ff6a57">Overall, the analysis underscores the need for improved NVS architectures that integrate temporal awareness, stronger object-level representations, and more efficient resource utilization to support reliable novel view synthesis in complex, real-world applications.</p><br class="jsx-22a9728020ff6a57"/><h2 class="jsx-22a9728020ff6a57">Glossary</h2><ul class="jsx-22a9728020ff6a57 text-list"><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">NVS</strong>: Novel View Synthesis</li><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">NeRF</strong>: Neural Radiance Fields</li><li class="jsx-22a9728020ff6a57"><strong class="jsx-22a9728020ff6a57">ZeroNVS</strong>: Zero-shot 360-degree synthesis</li></ul><br class="jsx-22a9728020ff6a57"/><h4 class="jsx-22a9728020ff6a57">Firi Berhane:<!-- --> <a href="https://www.linkedin.com/in/firi-berhane-7b3a05232/" class="jsx-22a9728020ff6a57 linkColor">LinkedIn</a></h4><h4 class="jsx-22a9728020ff6a57">Hana Kassie:<!-- --> <a href="https://www.linkedin.com/in/hana-kassie-19b326227/" target="_blank" class="jsx-22a9728020ff6a57 linkColor">LinkedIn</a></h4><h4 class="jsx-22a9728020ff6a57">Joshua Mekuriaw:<!-- --> <a href="https://www.linkedin.com/in/joshua-tadesse-aa8081225/" target="_blank" class="jsx-22a9728020ff6a57 linkColor">LinkedIn</a></h4><h4 class="jsx-22a9728020ff6a57">Kushal Kedia:<!-- --> <a href="https://www.linkedin.com/in/kushal-kedia/" target="_blank" class="jsx-22a9728020ff6a57 linkColor">LinkedIn</a></h4></section></div><aside class="jsx-22a9728020ff6a57 sidebar"><h3 class="jsx-22a9728020ff6a57">Other Research</h3><div class="jsx-22a9728020ff6a57 cards"><button class="jsx-22a9728020ff6a57 view-all-text-btn">View All Researches →</button></div></aside></article></main><!--$--><!--/$--><footer class="footer_footer__WFt8a"><div class="footer_left__pBTxI"><img alt="CVGRI Logo" loading="lazy" width="60" height="50" decoding="async" data-nimg="1" style="color:transparent" src="/favicon.ico"/><span class="footer_logoText__6RK_H">CVGRI</span></div><div class="footer_center__LK0Ix"><p>Made by CVGRI Team</p><p class="footer_year__1VT3T">2025</p></div></footer><script src="/cvgri.github.io/_next/static/chunks/webpack-cbc3379284357474.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[9766,[],\"\"]\n3:I[8924,[],\"\"]\n4:I[8605,[\"394\",\"static/chunks/394-6af1e34207d32b87.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"322\",\"static/chunks/app/research/page-ebbff567b58acdd6.js\"],\"default\"]\n5:I[1526,[\"394\",\"static/chunks/394-6af1e34207d32b87.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"322\",\"static/chunks/app/research/page-ebbff567b58acdd6.js\"],\"default\"]\n6:I[2072,[\"394\",\"static/chunks/394-6af1e34207d32b87.js\",\"250\",\"static/chunks/app/research/group4/page-090b1657dcc53b47.js\"],\"default\"]\n7:I[4431,[],\"OutletBoundary\"]\n9:I[5278,[],\"AsyncMetadataOutlet\"]\nb:I[4431,[],\"ViewportBoundary\"]\nd:I[4431,[],\"MetadataBoundary\"]\ne:\"$Sreact.suspense\"\n10:I[7150,[],\"\"]\n:HL[\"/cvgri.github.io/_next/static/css/c83f8f8090aa4c07.css\",\"style\"]\n:HL[\"/cvgri.github.io/_next/static/css/322b103938224f59.css\",\"style\"]\n:HL[\"/cvgri.github.io/_next/static/css/f71f4c7c3128a037.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"xR7L3hmiRj1pOA2XqAz5n\",\"p\":\"/cvgri.github.io\",\"c\":[\"\",\"research\",\"group4\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"research\",{\"children\":[\"group4\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/cvgri.github.io/_next/static/css/c83f8f8090aa4c07.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css\"}]}],[\"$\",\"body\",null,{\"className\":\"__variable_728e81 __variable_c4d517\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]]}],{\"children\":[\"research\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/cvgri.github.io/_next/static/css/322b103938224f59.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[[\"$\",\"$L4\",null,{}],[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"$L5\",null,{}]]]}],{\"children\":[\"group4\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L6\",null,{}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/cvgri.github.io/_next/static/css/f71f4c7c3128a037.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L7\",null,{\"children\":[\"$L8\",[\"$\",\"$L9\",null,{\"promise\":\"$@a\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],null],[\"$\",\"$Ld\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$e\",null,{\"fallback\":null,\"children\":\"$Lf\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$10\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n8:null\n"])</script><script>self.__next_f.push([1,"11:I[622,[],\"IconMark\"]\na:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"CVGRI 2025\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Developed by the CVGRI Team\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/cvgri.github.io/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"144x119\"}],[\"$\",\"$L11\",\"3\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"f:\"$a:metadata\"\n"])</script></body></html>